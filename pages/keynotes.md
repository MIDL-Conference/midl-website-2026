---
title: "Keynotes"
page_class: "keynotes"
---

{% from "_macros.html" import video %}

# Keynote speakers
<p><a id="keynoteX"></a></p>

## Prof. Ipek Oguz, Ph.D.![Portrait of Ipek Oguz](/images/IpekOguz.jpg){: width="180" }
### Associate Professor, Department of Electrical Engineering and Computer Science, Vanderbilt University
<!-- ### McGill & CIFAR AI Chair ‚Äì Mila

üè† [Homepage](https://www.cim.mcgill.ca/~arbel/)
üéì [Google Scholar](https://scholar.google.com/citations?user=0AN34NMAAAAJ&hl=en&oi=ao) -->
  
<!-- 
#### **The Promise of Causal Deep Learning for Personalized Medicine based on Medical Images: Towards Safe and Trustworthy Clinical Deployment**


**Abstract:** In current clinical practice, medical decisions are guided by broad demographic features and standardized clinical markers, which may not fully capture the nuances of the individual patients.  Personalized medicine, informed by medical images, has the potential to transform disease management by tailoring treatment decisions to the unique needs of individual patients, holding enormous potential to improve healthcare and drug development. Yet these models currently remain underexplored, and open challenges presented by real clinical contexts hinder their safe clinical deployment.  In this talk, I describe causal inference models for personalized medicine based on high dimensional medical images (MRI) acquired from patients with neurological disease, such as multiple sclerosis (MS), given their long-term, complex, heterogeneous evolutions and no cure. I will describe temporal causal models that permit learning the evolution of disease progression based from multimodal inputs.  Causal inference models then predict future individual factual and counterfactual progression trajectories on different treatments in latent space, as well as their associated individual treatment response trajectories. We then describe strategies to improve the safety and reliability of the models, such as building uncertainty-aware causal models for image-based personalized medicine. Extensive experiments large, multi-centre, randomized clinical trials for MS treatments illustrate the power of uncertainty-based causal inference frameworks to (a) accurately predict a patient‚Äôs future new lesional activity, continuous disability evolution, and treatment response to different drugs, and (b) discover subgroups of patients for which the models have high confidence in their response to treatments (even in clinical trials with no proven efficacy).


**Biography:**  Prof. Tal Arbel is a Professor in the Department of Electrical and Computer Engineering, where she is the Director of the Probabilistic Vision Group and Medical Imaging Lab in the Centre for Intelligent Machines, McGill University. She is a Canada CIFAR AI Chair - MILA (Montreal Institute for Learning Algorithms) and Associate Member of the Goodman Cancer Research Centre. Prof. Arbel's research focuses on development of probabilistic, deep learning methods in computer vision and medical image analysis, for a wide range of real-world applications involving neurological diseases. For example, the machine learning algorithms developed by her team for the detection and segmentation of lesions in brain MRI of patients with Multiple Sclerosis (MS) have been used in the clinical trial analysis of almost all the new MS drugs currently used worldwide. She is a recipient of the 2019 McGill Engineering Christophe Pierre Research Award. She regularly serves on the organizing team of major international conferences in computer vision and in medical image analysis (e.g. MICCAI, MIDL, ICCV, CVPR). She was an Associate Editor for IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), and Computer Vision and Image Understanding (CVIU). She is currently the Editor-in-Chief and co-founder of the arXiv overlay journal: Machine Learning for Biomedical Imaging (MELBA). -->

<!--{{ video("https://video.midl.io/2022/keynote1.mp4") }}-->
***
<p><a id="keynoteX"></a></p>

## 	Prof. Dean HO, Ph.D.![Portrait of Dean Ho](/images/deanho.avif){: width="180" }
### Provost‚Äôs Chair Professor, Department of Biomedical Engineering, National University of Singapore. 
### Director, The N.1 Institute for Health (N.1).
### Director, The Institute for Digital Medicine (WisDM)

<!-- 
üè† [Homepage](https://www.professoren.tum.de/rueckert-daniel)
üéì [Google Scholar](https://scholar.google.com/citations?user=H0O0WnQAAAAJ&hl=en&oi=ao)

#### **AI and the Future of Radiology**


**Abstract:**  Artificial Intelligence (AI) is changing many fields across science and our society. This talk will discuss how AI is changing medicine and healthcare, particularly in radiology. I will focus on how AI can support the acquisition of medical images and image analysis and interpretation. This can enable the early detection of diseases and support the improved personalised diagnosis. I will show several examples of this in the talk, including cardiovascular MR imaging. Furthermore, we will discuss how AI solutions can be privacy-preserving while also providing trustworthy and explainable solutions for clinicians.


**Biography:**  Prof. R√ºckert‚Äôs field of research is the area of Artificial Intelligence (AI) and Machine Learning and their application to medicine and healthcare. His research focuses on (1) the development of innovative algorithms for biomedical image acquisition, image analysis and image interpretation ‚Äì especially in the areas of image reconstruction, registration, segmentation, traching and modelling; (2) AI for extracting clinically useful information from biomedical images ‚Äì especially for computer-assisted diagnosis and prognosis. Since 2020, Daniel R√ºckert is Alexander von Humboldt Professor for AI in Medicine and Healthcare at the Technical University of Munich. He is also a Professor at Imperial College London. He gained a MSc from Technical University Berlin in 1993, a PhD from Imperial College in 1997, followed by a post-doc at King‚Äôs College London. In 1999 he joined Imperial College as a Lecturer, becoming Senior Lecturer in 2003 and full Professor in 2005. From 2016 to 2020 he served as Head of the Department of Computing at Imperial College. -->

<!--{{ video("https://video.midl.io/2022/keynote1.mp4") }}-->

***
<p><a id="keynoteX"></a></p>

## 	Prof. Junzhou Huang, Ph.D.![Portrait of Junzhou Huang](/images/junzhou.jpg){: width="180" }
###  Jenkins Garrett Professor, Department of Computer Science and Engineering, University of Texas at Arlington. 

<!-- üè† [Homepage](https://profiles.stanford.edu/mirabela-rusu)
üéì [Google Scholar](https://scholar.google.com/citations?user=vmwsOyoAAAAJ&hl=en&oi=ao) -->

<!-- #### **Multimodal Learning for Early Cancer Detection in Low Resource Settings** -->
<!-- #### **Bridging the Radiology-Pathology Gap for Prostate Cancer Characterization**  -->


<!-- **Abstract:**  Clinical care is inherently multimodal, with medical image data collected throughout the patient‚Äôs journey.  For example, a patient at risk of cancer will undergo an ultrasound-guided biopsy, and when available with MRI revealing regions to be targeted due to higher risk to harbor aggressive disease. This biopsy procedure seeks to collect tissue samples for pathology and will inform treatment strategies for best outcomes. This common scenario provides unique opportunities for Artificial Intelligence (AI) methods to effectively integrate multimodal data, and learn imaging signatures in patients with known outcomes, to enable early cancer detection for patients at risk. My research focuses on developing AI methods that bridge the gap between highly informative modalities, e.g., pathology or MRI, and lower resolution modalities, e.g., ultrasound. These methods rely on multimodal image registration, image feature fusion, or integration of patient-specific data and population-specific information and rely on AI approaches for effective integration. While the learning is done with multiple imaging modalities, the inference requires only the low-resolution modality, e.g., ubiquitous conventional ultrasound, with applications in low-resource settings. These methods are applied to detect cancer and its aggressive extent in various cancers, e.g. prostate, kidney, or breast.   -->


<!-- **Biography:** Prof. Rusu is an Assistant Professor, in the Department of Radiology, and, by courtesy, Department of Urology and Biomedical Data Science, at Stanford University, where she leads the Personalized Integrative Medicine Laboratory (PIMed). The PIMed Laboratory has a multi-disciplinary direction and focuses on developing analytic methods for biomedical data integration, with a particular interest in radiology-pathology fusion to facilitate radiology image labeling. The radiology-pathology fusion allows the creation of detailed spatial labels, that later on can be used as input for advanced machine learning, such as deep learning. The recent focus of the lab has been on applying deep learning methods to detect and differentiate aggressive from indolent prostate cancers on MRI using the pathology information (both labels and the image content), work that was recently published in Medical Physics and Medical Image Analysis Journals. Moreover, our project are interested in further develop these approaches for ultrasound images. Prof. Rusu received a Master of Engineering in Bioinformatics from the National Institute of Applied Sciences in Lyon, France. She continued her training at the University of Texas Health Science Center in Houston, where she received a Master of Science and PhD degree in Health Informatics for her work in biomolecular structural data integration of cryo-electron micrographs and X-ray crystallography models. During her postdoctoral training at Rutgers and Case Western Reserve University, Prof. Rusu has developed computational tools for the integration and interpretation of multi-modal medical imaging data and focused on studying prostate and lung cancers. Prior to joining Stanford, Prof. Rusu was a Lead Engineer and Medical Image Analysis Scientist at GE Global Research Niskayuna NY where she was involved in the development of analytic methods to characterize biological samples in microscopy images and pathologic conditions in MRI or CT.  -->

<!--{{ video("https://video.midl.io/2022/keynote1.mp4") }}-->

<!-- ***
<p><a id="keynoteX"></a></p>

## Dr. Ismail Baris Turkbey, M.D.![Portrait of Ismail Baris Turkey](/images/keynotes/baris_turkbey.jpg){: width="180" }
### Director of the Artificial Intelligence Resource (AIR), Senior Clinician
### Center for Cancer Research, National Cancer Institute (NCI), National Institutes of Health (NIH)

üè† [Homepage](https://ccr.cancer.gov/staff-directory/ismail-baris-turkbey#qt-staff_profile_tabs-ui-tabs6)
üéì [Google Scholar](https://scholar.google.com/citations?user=XiMbUboAAAAJ&hl=en)

#### **Guiding Clinical Decisions in Localized Prostate Cancer with AI** 


**Abstract:**  Artificial Intelligence (AI) has become a prominent area of research in oncologic imaging, including prostate cancer. Despite numerous academic studies, the clinical translation of AI for guiding decision-making in localized prostate cancer care remains underexplored. This presentation will outline the critical steps for successfully integrating AI into clinical workflows. Topics include the development and validation of AI models using diverse datasets, strategies for deployment, and the prospective use of imaging-based AI in clinical decision-making. Insights from clinical trials on AI implementation will be shared, along with preliminary findings on the prognostic capabilities of AI models in predicting treatment outcomes and long-term prognosis.


**Biography:** Dr. Turkbey‚Äôs main research interests focus on prostate cancer imaging (multiparametric MRI, PET CT), prostate biopsy techniques, focal therapy for prostate cancer and artificial intelligence (segmentation, decision support systems). Dr. Turkbey is the Head of the Artificial Intelligence Resource (AIR), which makes AI tools available to CCR investigators with the goal of developing better screening and detection methods or predictive markers for patients.

<!--{{ video("https://video.midl.io/2022/keynote1.mp4") }}-->

***
<p><a id="keynoteX"></a></p>

## Dr. Mert Sabuncu, Ph.D.![Portrait of Mert Sabuncu](/images/mert-sanbuncu.jpeg){: width="180" }
### Professor, School of Electrical and Computer Engineering, Cornell University and Cornell Tech
### Vice Chair of AI and Engineering Research, Department of Radiology, Weill Cornell Medicine
<!-- 
üè† [Homepage](https://www.aylward.org)
üéì [Google Scholar](https://scholar.google.com/citations?user=u1UdL4oAAAAJ&hl=en&oi=ao)


#### **MONAI For The Next Generation of Medical Technologies**

**Abstract:** MONAI is an open-source platform for medical AI that has been downloaded over 3.5M times, featured in hundreds of publications, used to win numerous medical image analysis challenges, and integrated into every major cloud-base healthcare platform as well as multiple regulatory-approved clinical products.   This presentation will explore the use of MONAI in the discovery and development of future medical products.  That future involves the intertwining of training, simulation, and execution to train and control robots, optimize equipment ergonomics, and inspire new forms of monitoring and intervention.  That future will be built on open source: MONAI, Omniverse, and the Holoscan SDK.

**Biography:**  Dr. Stephen Aylward is the Global Developer Relations Manager for Medical Technologies at NVIDIA. With 25+ years in NIH, DARPA, and DoD-funded medical imaging research and open-source platform development (ITK, VTK, 3D Slicer, and MONAI), he is now combining open-source software with accelerated computing to explore and support next gen, AI-enhanced, clinical products.  His interests range from intelligent ultrasound systems to autonomous surgical robots. He is also a MICCAI Fellow, an adjunct professor in Computer Science at UNC, and chair of the MONAI Advisory Board.  Previously, he was Senior Director of Strategic Initiatives at Kitware, and before founding Kitware‚Äôs office in North Carolina, he was a tenured professor in Radiology at UNC. -->

<!--{{ video("https://video.midl.io/2022/keynote1.mp4") }}-->
